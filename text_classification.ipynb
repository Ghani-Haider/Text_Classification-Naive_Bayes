{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import csv\r\n",
    "import pandas as pd\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_file(fileName):\r\n",
    "    dataset = pd.read_table(fileName, header=0, sep=\",\", encoding=\"unicode_escape\")\r\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# preprocess creates the term frequency matrix for the review data set\r\n",
    "def preprocess(data):\r\n",
    "    count_vectorizer = CountVectorizer()\r\n",
    "    data = count_vectorizer.fit_transform(data)\r\n",
    "    #tfidf_data = TfidfTransformer(use_idf=False).fit_transform(data)\r\n",
    "\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def learn_model(data,target):\r\n",
    "  \r\n",
    "    classifier = None\r\n",
    "    #Your custom implementation of NaiveBayes classifier will go here.\r\n",
    "    \r\n",
    "    # count vector matrix\r\n",
    "    count_vector = data.toarray()\r\n",
    "    # N, |V|\r\n",
    "    Total_documents, Vocabulary_size = len(count_vector), len(count_vector[0])\r\n",
    "    \r\n",
    "    # get all individual classes\r\n",
    "    classes = list(np.unique(target))\r\n",
    "    # P(c_i)\r\n",
    "    Prob_Classes = np.zeros(len(classes))\r\n",
    "\r\n",
    "    # P(w_i/c_j) - P(each_word / given a class) \r\n",
    "    Prob_wi_by_cj = np.zeros((len(classes),Vocabulary_size))\r\n",
    "\r\n",
    "    #\r\n",
    "    for each_class in range(len(classes)):\r\n",
    "        # count of documents that have been mapped to this category c_j\r\n",
    "        docs_with_c_j = []\r\n",
    "        for doc in range(len(count_vector)):\r\n",
    "            if(target.iloc[doc] == classes[each_class]):\r\n",
    "                docs_with_c_j.append(count_vector[doc])\r\n",
    "        # Prob(c) of given class - P(c_j)\r\n",
    "        Prob_Classes[each_class] = len(docs_with_c_j) / Total_documents\r\n",
    "\r\n",
    "        # calculate the total count of each word\r\n",
    "        count_wi_by_cj = np.sum(docs_with_c_j, axis=0) # Count(w_i, c_j) for all w_i\r\n",
    "        count_wi_by_cj += 1 # laplace smoothing\r\n",
    "        Sum_Prob_wi_by_cj = np.sum(count_wi_by_cj) # summation(count(w, c_j))\r\n",
    "        Prob_wi_by_cj[each_class] = count_wi_by_cj / Sum_Prob_wi_by_cj # P(w_i | c_j)\r\n",
    "\r\n",
    "    Class_Prob_Dict = dict()\r\n",
    "    for each_class in range(len(classes)):\r\n",
    "        Class_Prob_Dict[classes[each_class]] = Prob_Classes[each_class]\r\n",
    "    \r\n",
    "    classifier = (classes, Class_Prob_Dict, Prob_wi_by_cj) # c's, P(c), P(w_i|c_j)\r\n",
    "\r\n",
    "    return classifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def classify(classifier, testdata):\r\n",
    "    \r\n",
    "    predicted_val=[]\r\n",
    "    #Your code to classify test data using the learned model will go here\r\n",
    "    classes, Class_Prob_Dict, Prob_wi_by_cj=classifier\r\n",
    "    test=testdata.toarray()\r\n",
    "    for row in range(len(test)): #for each doc\r\n",
    "        prob=[0]*len(classes) #probability of each class for one doc\r\n",
    "        for c in range(len(classes)):\r\n",
    "            #P(c|d)=P(c)*P(w|c)\r\n",
    "            prob[c]=Class_Prob_Dict[classes[c]]*Prob_wi_by_cj[c][test[row].astype(bool)].prod()\r\n",
    "        max_index=0\r\n",
    "        # adding the class with highest probability\r\n",
    "        max_prob=prob[0]\r\n",
    "        for p in range(len(prob)):\r\n",
    "            if prob[p]>max_prob:\r\n",
    "                max_index = p \r\n",
    "                max_prob=prob[p]\r\n",
    "        predicted_val.append(classes[max_index])\r\n",
    "\r\n",
    "    return predicted_val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def evaluate(actual_class, predicted_class):\r\n",
    "        \r\n",
    "    accuracy = -1    \r\n",
    "    #Your code to evaluate the model will go here. The code will print overall model's accuracy and precision \r\n",
    "    #and recall for each class label.\r\n",
    "    classes = list(np.unique(actual_class))\r\n",
    "    confusion_matrix = np.zeros((len(classes), len(classes)))\r\n",
    "    \r\n",
    "    # converting into dataframe\r\n",
    "    predicted_class = pd.Series(predicted_class)\r\n",
    "    \r\n",
    "    # confusion matrix\r\n",
    "    for actual in range(len(classes)):\r\n",
    "        for predicted in range(len(classes)):\r\n",
    "            for c in range(len(actual_class)):\r\n",
    "                if(actual_class.iloc[c] == classes[actual] and predicted_class.iloc[c] == classes[predicted]):\r\n",
    "                    confusion_matrix[actual][predicted] += 1\r\n",
    "\r\n",
    "    # accuracy\r\n",
    "    total = 0\r\n",
    "    diagonal = 0\r\n",
    "    for i in range(len(confusion_matrix)):\r\n",
    "        for j in range(len(confusion_matrix)):\r\n",
    "            if(i == j):\r\n",
    "                diagonal += confusion_matrix[i][j]\r\n",
    "            total += confusion_matrix[i][j]\r\n",
    "    accuracy = diagonal / total\r\n",
    "    \r\n",
    "    # recall\r\n",
    "    recall_lst = [0] * len(classes)\r\n",
    "    for i in range(len(classes)):\r\n",
    "        tp = confusion_matrix[i][i]\r\n",
    "        denominator = sum(confusion_matrix[i])\r\n",
    "        recall_lst[i] = tp / denominator\r\n",
    "\r\n",
    "    # precision\r\n",
    "    precision_lst = [0] * len(classes)\r\n",
    "    for i in range(len(classes)):\r\n",
    "        tp = confusion_matrix[i][i]\r\n",
    "        denominator = 0\r\n",
    "        for k in range(len(classes)):\r\n",
    "            denominator += confusion_matrix[k][i]\r\n",
    "        \r\n",
    "        if(denominator == 0):\r\n",
    "            precision_lst[i] = 0\r\n",
    "        else:\r\n",
    "            precision_lst[i] = tp / denominator\r\n",
    "\r\n",
    "    # f-measure\r\n",
    "    f_measure_lst = [0] * len(classes)\r\n",
    "    for i in range(len(classes)):\r\n",
    "        if(precision_lst[i] + recall_lst[i] == 0):\r\n",
    "            f_measure_lst[i] = 0\r\n",
    "        else:\r\n",
    "            f_measure_lst[i] = (2*precision_lst[i]*recall_lst[i])/(precision_lst[i]+recall_lst[i])\r\n",
    "\r\n",
    "    print(confusion_matrix)\r\n",
    "    print(\"The accuracy score is :\", accuracy)\r\n",
    "    print(\"The recall score is :\", recall_lst)\r\n",
    "    print(\"The precision score is :\", precision_lst)\r\n",
    "    print(\"The f-mesaure is :\", f_measure_lst)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "features = [\"SUMMARY\", \"categories\", \"sub_categories\"]\r\n",
    "\r\n",
    "print(\"Loading data.....\")\r\n",
    "dataset = load_file(\"TextClassification_Data.csv\")\r\n",
    "data,target = dataset[features[0]].fillna(\" \"), dataset[features[1]]\r\n",
    "\r\n",
    "print(\"preprocessing data.....\")\r\n",
    "word_vectors = preprocess(data)\r\n",
    "\r\n",
    "trainingX,testX,trainingY,testY = train_test_split(word_vectors,target,test_size=0.4,random_state=43)\r\n",
    "\r\n",
    "print(\"Learning model.....\")\r\n",
    "model = learn_model(trainingX,trainingY)\r\n",
    "\r\n",
    "print(\"Classifying test data......\")      \r\n",
    "predictedY = classify(model, testX)\r\n",
    "\r\n",
    "print(\"Evaluating results.....\")\r\n",
    "evaluate(testY,predictedY)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data.....\n",
      "preprocessing data.....\n",
      "Learning model.....\n",
      "Classifying test data......\n",
      "Evaluating results.....\n",
      "[[4.196e+03 4.210e+02 0.000e+00 7.200e+01 4.780e+02 4.420e+02]\n",
      " [3.200e+02 2.912e+03 0.000e+00 1.000e+02 6.310e+02 6.780e+02]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 9.000e+00 1.000e+00]\n",
      " [1.150e+02 1.350e+02 0.000e+00 1.085e+03 2.890e+02 1.050e+02]\n",
      " [3.430e+02 4.250e+02 0.000e+00 1.020e+02 3.044e+03 9.690e+02]\n",
      " [1.580e+02 3.940e+02 0.000e+00 1.050e+02 4.710e+02 4.912e+03]]\n",
      "The accuracy score is : 0.7048271648044693\n",
      "The recall score is : [0.7480834373328579, 0.6274509803921569, 0.0, 0.6275303643724697, 0.623387261929142, 0.8132450331125828]\n",
      "The precision score is : [0.8176149649259548, 0.6792628878003266, 0, 0.7411202185792349, 0.6184477854530679, 0.6911495708456451]\n",
      "The f-mesaure is : [0.7813052788380969, 0.6523297491039426, 0, 0.6796116504854369, 0.6209077001529832, 0.7472427169696508]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}